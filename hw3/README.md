# Flappy Bird — 基于DQN的Flappy Bird智能体训练

## 项目简介

本项目基于深度强化学习中的 Deep Q-Network (DQN) 算法，实现了一个智能体自动玩 Flappy Bird 游戏。通过训练，智能体能够学习在游戏中控制小鸟躲避管道障碍，尽可能长时间生存并获得更高的分数。

游戏环境基于开源项目 [DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird) ，我利用项目的游戏环境实现了强化学习训练代码和模型。

---

## 算法说明

### 深度强化学习：DQN (Deep Q-Network)

DQN 是一种结合了 Q 学习和深度神经网络的强化学习方法，使用神经网络近似 Q 函数，输入状态，输出每个动作的 Q 值，指导智能体选择动作。

- **状态输入**: 当前游戏画面（像素图像或预处理图像）
- **动作空间**: 2个动作 — 不操作（不跳），跳跃（flap）
- **奖励设计**: 活着持续获得微小正奖励，碰撞失败获得负奖励

### 神经网络结构（卷积层参数）

网络主要由卷积层和全连接层组成，用于处理游戏画面图像，提取有效特征：

| 层类型   | 参数详情                   | 输出大小    |
|-------|------------------------|-------------------------------|
| 输入层   | 游戏画面灰度图，连续4帧           | (80, 80, 4)                   |
| 卷积层1  | 32个卷积核，大小8x8，步长4，ReLU  | (19, 19, 32)               |
| 卷积层2  | 64个卷积核，大小4x4，步长2，ReLU  | (8, 8, 64)                  |
| 卷积层3  | 64个卷积核，大小3x3，步长1，ReLU  | (6, 6, 64)                 |
| 展平层   | 展开卷积输出为一维向量            | 64 * 6 * 6 = 2304             |
| 全连接层1 | 512个神经元，ReLU           | 512                           |
| 全连接层2 | 2个动作的Q值                | 2                             |


### 训练细节

- **优化器（optimizer）**：Adam
- **学习率（lr）**：0.0001
- **折扣因子 γ（gamma）**：0.99，表示未来奖励的重要性
- **ε-贪心策略**：
  - 初始探索率 ε（initial_epsilon）：0.3（表示初期有30%的概率随机探索）
  - 最终探索率 ε（final_epsilon）：0.001（训练后期最低随机概率）
  - 探索衰减步数（explore_steps）：200,000 步，从初始ε逐渐衰减至最终ε
- **经验回放缓冲区大小（replay_memory_size）**：100,000 条经验
- **Mini-batch 大小（batch_size）**：128，每次训练采样的经验数量
- **训练流程**：
  - 观察阶段（observe_steps）：20,000 步，期间仅收集经验，不进行训练
  - 最大训练步数（max_steps）：1,000,000 步
- **模型保存与日志**：
  - 保存间隔（save_interval）：每 50,000 步保存一次模型
  - 日志打印间隔（log_interval）：每 5,000 步打印一次训练状态

---

## 环境与依赖

本项目基于 Python 3.11，主要依赖如下：

- PyTorch 2.6.0
- numpy
- pygame
- opencv-python
- tqdm

详细依赖见 `requirements.txt`

---

## 模型演示与 GIF 动图生成

本项目提供了一个演示脚本 `Run_model.py`，用于运行训练好的 DQN 模型，并将智能体在 Flappy Bird 游戏中的表现录制为动态图片（GIF）。该功能便于直观观察模型在游戏环境中的决策行为和性能。

### 说明

- `Run_model.py`：主脚本，加载训练好的模型，运行游戏，并保存游戏过程为 GIF。
- `saved_models/`：默认的模型保存文件夹，包含形如 `flappy_bird_dqn_100000.pth` 的训练权重文件。
- `GIF/`：GIF 动图保存文件夹，由脚本自动创建。

---

### 配置参数

| 参数            | 描述                                      | 默认值                              |
|-----------------|-------------------------------------------|-------------------------------------|
| `MODEL_PATH`    | 要加载的训练模型路径                      | `saved_models/flappy_bird_dqn_100000.pth` |
| `MAX_FRAMES`    | 录制的最大帧数（30帧/秒，180帧约6秒）     | `180`                               |

> 注意：GIF 名称会根据模型文件名自动命名，如模型为 `flappy_bird_dqn_100000.pth`，则生成的动图为 `GIF/FB_100000.gif`。

---

## 代码引用说明

本项目的 Flappy Bird 游戏环境代码来自于开源仓库：

[DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird)

---

